{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a56478",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install nltk, emoji, tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd346944",
   "metadata": {},
   "source": [
    "# –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5544352",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import html\n",
    "import emoji\n",
    "from tqdm import tqdm\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7493e78",
   "metadata": {},
   "source": [
    "—Ä—É—Å—Å–∫–∏–π —Ç–µ–∫—Å—Ç"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17af84ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"punkt_tab\")\n",
    "\n",
    "stemmer = SnowballStemmer(\"russian\")\n",
    "stop_words = set(stopwords.words(\"russian\"))\n",
    "\n",
    "tqdm.pandas() \n",
    "\n",
    "# —á–∏—Å—Ç–∫–∞\n",
    "def clean_text(text):\n",
    "    if pd.isnull(text):\n",
    "        return \"\"\n",
    "    \n",
    "    text = html.unescape(text)\n",
    "    text = re.sub(r\"<.*?>\", \" \", text)\n",
    "    # text = re.sub(r\"</?[^>]+>\", \" \", text)\n",
    "    text = re.sub(r\"[^a-zA-Z–∞-—è–ê-–Ø]\", \" \", text)\n",
    "\n",
    "    return text.lower()\n",
    "\n",
    "# —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –∏ —Å—Ç–µ–º–º–∏–Ω–≥\n",
    "def tokenize_and_stem(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [t for t in tokens if t not in stop_words and len(t) > 2]\n",
    "\n",
    "    stems = [stemmer.stem(t) for t in tokens]\n",
    "\n",
    "    return \" \".join(stems)\n",
    "\n",
    "def prepare_text(df):\n",
    "    print(\"–û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤ –Ω–∞—á–∞–ª–∞—Å—å...\")\n",
    "    df[\"text_clean\"] = df[\"text\"].progress_apply(lambda x: tokenize_and_stem(clean_text(x)))\n",
    "    print(\"–û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞!\")\n",
    "    return df\n",
    "\n",
    "df = prepare_text(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55b5352",
   "metadata": {},
   "source": [
    "—Ä—É—Å—Å–∫–∏–π –∏ –∞–Ω–≥–ª–∏–π—Å–∫–∏–π —Ç–µ–∫—Å—Ç—ã"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74237a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"punkt_tab\")\n",
    "\n",
    "stop_words = set(stopwords.words(\"russian\")) | set(stopwords.words(\"english\"))\n",
    "\n",
    "stem_ru = SnowballStemmer(\"russian\")\n",
    "stem_en = SnowballStemmer(\"english\")\n",
    "\n",
    "def stem_word(word):\n",
    "    if re.fullmatch(r\"[a-zA-Z]+\", word):\n",
    "        return stem_en.stem(word)\n",
    "    elif re.fullmatch(r\"[–∞-—è–ê-–Ø—ë–Å]+\", word):\n",
    "        return stem_ru.stem(word)\n",
    "    else:\n",
    "        return word\n",
    "    \n",
    "\n",
    "tqdm.pandas() \n",
    "\n",
    "# —á–∏—Å—Ç–∫–∞\n",
    "def clean_text(text):\n",
    "    if pd.isnull(text):\n",
    "        return \"\"\n",
    "    \n",
    "    text = html.unescape(text)\n",
    "    text = re.sub(r\"<.*?>\", \" \", text)\n",
    "    text = re.sub(r\"[^a-zA-Z–∞-—è–ê-–Ø]\", \" \", text)\n",
    "\n",
    "    return text.lower()\n",
    "\n",
    "# —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –∏ —Å—Ç–µ–º–º–∏–Ω–≥\n",
    "def tokenize_and_stem(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [t for t in tokens if t not in stop_words and len(t) > 2]\n",
    "\n",
    "    stems = [stem_word(t.lower()) for t in tokens]\n",
    "\n",
    "    return \" \".join(stems)\n",
    "\n",
    "def prepare_text(df):\n",
    "    print(\"–û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤ –Ω–∞—á–∞–ª–∞—Å—å...\")\n",
    "    df[\"text_clean\"] = df[\"text\"].progress_apply(lambda x: tokenize_and_stem(clean_text(x)))\n",
    "    print(\"–û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞!\")\n",
    "    return df\n",
    "\n",
    "df = prepare_text(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852b3b89",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ffb3707",
   "metadata": {},
   "source": [
    "## –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da83ead",
   "metadata": {},
   "source": [
    "TF-IDF-vector + SVD + K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3c23a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import html\n",
    "import emoji\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50de2edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('TextsUnlabeledVar1.xlsx', header=None, names=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a06ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"punkt_tab\")\n",
    "\n",
    "stemmer = SnowballStemmer(\"russian\")\n",
    "stop_words = set(stopwords.words(\"russian\")) | set(stopwords.words(\"english\"))\n",
    "\n",
    "tqdm.pandas() \n",
    "\n",
    "# —á–∏—Å—Ç–∫–∞\n",
    "def clean_text(text):\n",
    "    if pd.isnull(text):\n",
    "        return \"\"\n",
    "    \n",
    "    text = html.unescape(text)\n",
    "    text = re.sub(r\"<.*?>\", \" \", text)\n",
    "    text = re.sub(r\"[^a-zA-Z–∞-—è–ê-–Ø]\", \" \", text)\n",
    "\n",
    "    return text.lower()\n",
    "\n",
    "# —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –∏ —Å—Ç–µ–º–º–∏–Ω–≥\n",
    "def tokenize_and_stem(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [t for t in tokens if t not in stop_words and len(t) > 2]\n",
    "\n",
    "    stems = [stemmer.stem(t) for t in tokens]\n",
    "\n",
    "    return \" \".join(stems)\n",
    "\n",
    "def prepare_text(df):\n",
    "    print(\"–û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤ –Ω–∞—á–∞–ª–∞—Å—å..\")\n",
    "    df[\"text_clean\"] = df[\"text\"].progress_apply(lambda x: tokenize_and_stem(clean_text(x)))\n",
    "    print(\"–û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞!\")\n",
    "    return df\n",
    "\n",
    "df = prepare_text(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b0f2e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a4b359",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(max_df=0.9, min_df=5, ngram_range=(1,2), max_features=100000)\n",
    "X = vectorizer.fit_transform(df[\"text_clean\"])\n",
    "\n",
    "svd = TruncatedSVD(n_components=100, random_state=42)\n",
    "X_svd = svd.fit_transform(X) \n",
    "\n",
    "kmeans = KMeans(n_clusters=2, random_state=42, n_init=10)\n",
    "labels = kmeans.fit_predict(X_svd)\n",
    "\n",
    "df['cluster'] = labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f015f083",
   "metadata": {},
   "source": [
    "## –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b07c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import make_scorer, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83738ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_texts = df['text_clean']\n",
    "y = df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d17c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(\n",
    "    max_df=0.9,\n",
    "    min_df=5,\n",
    "    ngram_range=(1,2),\n",
    "    max_features=10000,\n",
    ")\n",
    "\n",
    "X = vectorizer.fit_transform(X_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42319153",
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "scorer = make_scorer(f1_score)\n",
    "\n",
    "models = [\n",
    "    (\"LogisticRegression\", LogisticRegression(\n",
    "        penalty='l2',\n",
    "        C=1.0,\n",
    "        solver='liblinear',\n",
    "        max_iter=1000,\n",
    "        random_state=42\n",
    "    )),\n",
    "    (\"RandomForest\", RandomForestClassifier(\n",
    "        n_estimators=300,\n",
    "        max_depth=None,\n",
    "        min_samples_split=2,\n",
    "        min_samples_leaf=1,\n",
    "        max_features='sqrt',\n",
    "        bootstrap=True,\n",
    "        random_state=42\n",
    "    )),\n",
    "    (\"SVM\", SVC(\n",
    "        kernel='linear',\n",
    "        C=1.0,\n",
    "        probability=True,\n",
    "        random_state=42\n",
    "    ))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6f6931",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, model in models:\n",
    "    scores = cross_val_score(model, X, y, cv=skf, scoring=scorer)\n",
    "    print(f\"–ú–æ–¥–µ–ª—å: {name}\")\n",
    "    print(f\"F1-score –ø–æ 5 —Ñ–æ–ª–¥–∞–º: {scores}\")\n",
    "    print(f\"–°—Ä–µ–¥–Ω–∏–π F1: {scores.mean():.4f}\")\n",
    "    print(\"-\"*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af975685",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_texts = df['text_clean']\n",
    "y = df['label']\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_df=0.9,\n",
    "    min_df=5,\n",
    "    ngram_range=(1,2),\n",
    "    max_features=10000,\n",
    ")\n",
    "X = vectorizer.fit_transform(X_texts)\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "scorer = make_scorer(f1_score)\n",
    "\n",
    "models = [\n",
    "    (\"LogisticRegression\", LogisticRegression(\n",
    "        penalty='l2',\n",
    "        C=1.0,\n",
    "        solver='liblinear',\n",
    "        max_iter=1000,\n",
    "        random_state=42\n",
    "    )),\n",
    "    (\"RandomForest\", RandomForestClassifier(\n",
    "        n_estimators=300,\n",
    "        max_depth=None,\n",
    "        min_samples_split=2,\n",
    "        min_samples_leaf=1,\n",
    "        max_features='sqrt',\n",
    "        bootstrap=True,\n",
    "        random_state=42\n",
    "    )),\n",
    "    (\"SVM\", SVC(\n",
    "        kernel='linear',\n",
    "        C=1.0,\n",
    "        probability=True,\n",
    "        random_state=42\n",
    "    ))\n",
    "]\n",
    "\n",
    "for name, model in models:\n",
    "    scores = cross_val_score(model, X, y, cv=skf, scoring=scorer)\n",
    "    print(f\"–ú–æ–¥–µ–ª—å: {name}\")\n",
    "    print(f\"F1-score –ø–æ 5 —Ñ–æ–ª–¥–∞–º: {scores}\")\n",
    "    print(f\"–°—Ä–µ–¥–Ω–∏–π F1: {scores.mean():.4f}\")\n",
    "    print(\"-\"*40)\n",
    "\n",
    "final_model = LogisticRegression(\n",
    "    penalty='l2',\n",
    "    C=1.0,\n",
    "    solver='liblinear',\n",
    "    max_iter=1000,\n",
    "    random_state=42\n",
    ")\n",
    "final_model.fit(X, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ddf1aa",
   "metadata": {},
   "source": [
    "## –†–µ–≥—Ä–µ—Å—Å–∏—è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8390f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import make_scorer, mean_squared_error\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8a8852",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_texts = df['text_clean']\n",
    "y = df['target']  # —á–∏—Å–ª–æ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ed0d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(\n",
    "    max_df=0.9,\n",
    "    min_df=5,\n",
    "    ngram_range=(1,2),\n",
    "    max_features=10000,\n",
    "    sublinear_tf=True,\n",
    "    strip_accents='unicode'\n",
    ")\n",
    "X = vectorizer.fit_transform(X_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967ac78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "mse_scorer = make_scorer(mean_squared_error, greater_is_better=False)\n",
    "\n",
    "models = [\n",
    "    (\"LinearRegression\", LinearRegression()),\n",
    "    (\"Ridge\", Ridge(alpha=1.0, solver='auto', random_state=42)),\n",
    "    (\"Lasso\", Lasso(alpha=0.01, max_iter=1000, random_state=42)),\n",
    "    (\"RandomForest\", RandomForestRegressor(\n",
    "        n_estimators=300,\n",
    "        max_depth=None,\n",
    "        min_samples_split=2,\n",
    "        min_samples_leaf=1,\n",
    "        max_features='sqrt',\n",
    "        bootstrap=True,\n",
    "        random_state=42\n",
    "    )),\n",
    "    (\"SVR\", SVR(\n",
    "        kernel='linear',\n",
    "        C=1.0,\n",
    "        epsilon=0.1\n",
    "    ))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb265dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, model in models:\n",
    "    scores = cross_val_score(model, X, y, cv=kf, scoring=mse_scorer)\n",
    "    print(f\"–ú–æ–¥–µ–ª—å: {name}\")\n",
    "    print(f\"MSE –ø–æ 5 —Ñ–æ–ª–¥–∞–º: {-scores}\")  # —É–º–Ω–æ–∂–∞–µ–º –Ω–∞ -1, –ø–æ—Ç–æ–º—É —á—Ç–æ greater_is_better=False\n",
    "    print(f\"–°—Ä–µ–¥–Ω–∏–π MSE: {-scores.mean():.4f}\")\n",
    "    print(\"-\"*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582ecebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import make_scorer, mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "X_texts = df['text_clean']\n",
    "y = df['target']  # —á–∏—Å–ª–æ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_df=0.9,\n",
    "    min_df=5,\n",
    "    ngram_range=(1,2),\n",
    "    max_features=10000,\n",
    "    sublinear_tf=True,\n",
    "    strip_accents='unicode'\n",
    ")\n",
    "X = vectorizer.fit_transform(X_texts)\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "mse_scorer = make_scorer(mean_squared_error, greater_is_better=False)\n",
    "\n",
    "models = [\n",
    "    (\"LinearRegression\", LinearRegression()),\n",
    "    (\"Ridge\", Ridge(alpha=1.0, solver='auto', random_state=42)),\n",
    "    (\"Lasso\", Lasso(alpha=0.01, max_iter=1000, random_state=42)),\n",
    "    (\"RandomForest\", RandomForestRegressor(\n",
    "        n_estimators=300,\n",
    "        max_depth=None,\n",
    "        min_samples_split=2,\n",
    "        min_samples_leaf=1,\n",
    "        max_features='sqrt',\n",
    "        bootstrap=True,\n",
    "        random_state=42\n",
    "    )),\n",
    "    (\"SVR\", SVR(\n",
    "        kernel='linear',\n",
    "        C=1.0,\n",
    "        epsilon=0.1\n",
    "    ))\n",
    "]\n",
    "\n",
    "for name, model in models:\n",
    "    scores = cross_val_score(model, X, y, cv=kf, scoring=mse_scorer)\n",
    "    print(f\"–ú–æ–¥–µ–ª—å: {name}\")\n",
    "    print(f\"MSE –ø–æ 5 —Ñ–æ–ª–¥–∞–º: {-scores}\")  # —É–º–Ω–æ–∂–∞–µ–º –Ω–∞ -1, –ø–æ—Ç–æ–º—É —á—Ç–æ greater_is_better=False\n",
    "    print(f\"–°—Ä–µ–¥–Ω–∏–π MSE: {-scores.mean():.4f}\")\n",
    "    print(\"-\"*40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b74026",
   "metadata": {},
   "source": [
    "## –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc774788",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# X_tfidf ‚Äî —Ç–≤–æ—è TF-IDF –º–∞—Ç—Ä–∏—Ü–∞ (—Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–∞—è)\n",
    "# vectorizer ‚Äî —Ç–≤–æ–π TfidfVectorizer\n",
    "\n",
    "# 1. —Å—É–º–º–∏—Ä—É–µ–º TF-IDF –ø–æ –≤—Å–µ–º –¥–æ–∫—É–º–µ–Ω—Ç–∞–º\n",
    "tfidf_sum = X.sum(axis=0)  # shape = (1, num_features)\n",
    "\n",
    "# 2. –∏–º–µ–Ω–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (—Å–ª–æ–≤)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# 3. —Å–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —É–±—ã–≤–∞–Ω–∏—é\n",
    "top_idx = np.array(tfidf_sum).ravel().argsort()[::-1]\n",
    "\n",
    "# 4. –≤—ã–≤–æ–¥–∏–º —Ç–æ–ø N —Å–ª–æ–≤\n",
    "N = 50\n",
    "top_words = [(feature_names[i], tfidf_sum[0, i]) for i in top_idx[:N]]\n",
    "\n",
    "print(\"–¢–æ–ø-50 —Å–ª–æ–≤ –ø–æ –≤—Å–µ–º—É –∫–æ—Ä–ø—É—Å—É:\")\n",
    "for word, score in top_words:\n",
    "    print(word, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5addb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_unique_words = len(vectorizer.vocabulary_)\n",
    "print(n_unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0019a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = vectorizer.get_feature_names_out()\n",
    "print(len(feature_names))\n",
    "print(feature_names[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efdd719c",
   "metadata": {},
   "source": [
    "# –í—Å—ë –æ—Å—Ç–∞–ª—å–Ω–æ–µ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c190c2e9",
   "metadata": {},
   "source": [
    "## –ù–∞ —Å–ª—É—á–∞–π –Ω–µ–≤–æ–∑–º–æ–∂–Ω–æ–≥–æ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10dfb985",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ø–µ—Ä–µ–≤–æ–¥ —Å–º–∞–π–ª–∏–∫–æ–≤ –≤ —Ç–µ–∫—Å—Ç\n",
    "def replace_emoji(text):\n",
    "    return emoji.demojize(text, language=\"ru\")\n",
    "\n",
    "\n",
    "# —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å\n",
    "POS_EMOJI = \" –ø–æ–∑–∏—Ç–∏–≤ \"\n",
    "NEG_EMOJI = \" –Ω–µ–≥–∞—Ç–∏–≤ \"\n",
    "\n",
    "def replace_emoji_with_sentiment(text):\n",
    "    for ch in text:\n",
    "        if ch in emoji.EMOJI_DATA:\n",
    "            if ch in {\"üòÄ\", \"üòÉ\", \"üòä\", \"üòç\", \"üòÅ\", \"üòÇ\", \"ü§£\", \"‚ù§Ô∏è\", \"üëç\"}:\n",
    "                text = text.replace(ch, POS_EMOJI)\n",
    "            elif ch in {\"üò°\", \"üò†\", \"üò¢\", \"üò≠\", \"üëé\", \"üíî\"}:\n",
    "                text = text.replace(ch, NEG_EMOJI)\n",
    "            else:\n",
    "                text = text.replace(ch, \" EMOJI_NEUTRAL \")\n",
    "    return text\n",
    "\n",
    "# —á–∏—Å—Ç–∫–∞\n",
    "def clean_text(text):\n",
    "    if pd.isnull(text):\n",
    "        return \"\"\n",
    "    \n",
    "    text = html.unescape(text)\n",
    "    text = re.sub(r\"<.*?>\", \" \", text)\n",
    "    #text = replace_emoji_with_sentiment(text)\n",
    "    text = re.sub(r\"[^a-zA-Z–∞-—è–ê-–Ø0-9]\", \" \", text)\n",
    "\n",
    "    return text.lower()\n",
    "\n",
    "# —É–¥–∞–ª–µ–Ω–∏–µ —á–∏—Å–µ–ª\n",
    "text = re.sub(r\"[^a-zA-Z–∞-—è–ê-–Ø]\", \" \", text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af849249",
   "metadata": {},
   "source": [
    "## –ú—É—Å–æ—Ä"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d72ba0",
   "metadata": {},
   "source": [
    "### –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309a2405",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = vectorizer.get_feature_names_out()\n",
    "print(len(feature_names))\n",
    "print(feature_names[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1eb7372",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_vector = X[0]\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "sorted_items = sorted(zip(feature_names, doc_vector.toarray()[0]), key=lambda x: x[1], reverse=True)\n",
    "print(sorted_items[:50]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3528990",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = vectorizer.transform(df[\"text_clean\"])  # TF-IDF –º–∞—Ç—Ä–∏—Ü–∞\n",
    "\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# —Å—Ä–µ–¥–Ω–∏–π TF-IDF –∫–∞–∂–¥–æ–≥–æ —Å–ª–æ–≤–∞ –ø–æ –≤—Å–µ–º –¥–æ–∫—É–º–µ–Ω—Ç–∞–º\n",
    "mean_tfidf = np.asarray(X.mean(axis=0)).ravel()\n",
    "\n",
    "# —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞\n",
    "sorted_features = sorted(\n",
    "    zip(feature_names, mean_tfidf),\n",
    "    key=lambda x: x[1],\n",
    "    reverse=True\n",
    ")\n",
    "\n",
    "# —Ç–æ–ø-20 —Å–∞–º—ã—Ö \"–≤–∞–∂–Ω—ã—Ö\" —Å–ª–æ–≤ –∫–æ—Ä–ø—É—Å–∞\n",
    "print(sorted_features[:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e165721e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "top_words = {}\n",
    "\n",
    "for cluster_id in np.unique(labels):\n",
    "    idxs = np.where(labels == cluster_id)[0]\n",
    "    tfidf_sum = X[idxs].sum(axis=0)\n",
    "\n",
    "    # —Ç–æ–ø-10 —Å–ª–æ–≤\n",
    "    top_idx = np.array(tfidf_sum).ravel().argsort()[::-1][:10]\n",
    "    top_words[cluster_id] = [feature_names[i] for i in top_idx]\n",
    "\n",
    "print(\"–¢–æ–ø-10 —Å–ª–æ–≤ –ø–æ –∫–ª–∞—Å—Ç–µ—Ä–∞–º:\")\n",
    "for cid, words in top_words.items():\n",
    "    print(f\"–ö–ª–∞—Å—Ç–µ—Ä {cid}: {words}\")\n",
    "\n",
    "\n",
    "\n",
    "cluster_reviews = None\n",
    "for cid, words in top_words.items():\n",
    "    if any(w in words for w in ['–æ—Ç–µ–ª','–Ω–æ–º–µ—Ä','–∑–∞–≤—Ç—Ä–∞–∫','–ø–µ—Ä—Å–æ–Ω–∞']):\n",
    "        cluster_reviews = cid\n",
    "        break\n",
    "\n",
    "\n",
    "num_reviews = (df['cluster'] == cluster_reviews).sum()\n",
    "print(f\"–û—Ü–µ–Ω–∫–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –æ—Ç–∑—ã–≤–æ–≤: {num_reviews}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ee03cd",
   "metadata": {},
   "source": [
    "### –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc3db9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "stop_words = set(stopwords.words(\"russian\")) | set(stopwords.words(\"english\"))\n",
    "\n",
    "# –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –Ω–∞–∑–≤–∞–Ω–∏–π\n",
    "def preprocess_app_name(name):\n",
    "    if pd.isnull(name):\n",
    "        return \"\"\n",
    "    return re.sub(r\"[^a-zA-Z–∞-—è–ê-–Ø0-9]\", \" \", name).lower().strip()\n",
    "\n",
    "# —á—Ç–æ –≤ –∏—Ç–æ–≥–µ –ø–æ–ª—É—á–∞–µ—Ç—Å—è\n",
    "def prepare_text(df):\n",
    "    df[\"app_name_clean\"] = df[\"app_name\"].apply(preprocess_app_name)\n",
    "    df[\"desc_clean\"] = df[\"shortDescription\"].fillna(\"\") + \" \" + df[\"full_description\"].fillna(\"\")\n",
    "    df[\"desc_clean\"] = df[\"desc_clean\"].apply(lambda x: tokenize_and_stem(clean_text(x)))\n",
    "    df[\"text_clean\"] = df[\"app_name_clean\"] + \" \" + df[\"desc_clean\"]\n",
    "    return df\n",
    "'''\n",
    "\n",
    "'''\n",
    "nltk==3.8.1\n",
    "nltk.download(\"punkt\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22a01a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# –ü—Ä–∏–º–µ—Ä —Ç–µ–∫—Å—Ç–æ–≤\n",
    "texts = [\n",
    "    \"–æ—Ç–µ–ª—å —Ö–æ—Ä–æ—à–∏–π –Ω–æ–º–µ—Ä\",\n",
    "    \"–Ω–æ–º–µ—Ä –±—ã–ª —á–∏—Å—Ç—ã–π\",\n",
    "    \"–æ—Ç–µ–ª—å —á–∏—Å—Ç—ã–π –∏ —É—é—Ç–Ω—ã–π\"\n",
    "]\n",
    "\n",
    "# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è CountVectorizer\n",
    "vectorizer = CountVectorizer(\n",
    "    max_df=0.9,        # –∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞—Ç—å —Å–ª–∏—à–∫–æ–º —á–∞—Å—Ç—ã–µ —Å–ª–æ–≤–∞\n",
    "    min_df=1,          # –∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞—Ç—å —Å–ª–æ–≤–∞, –≤—Å—Ç—Ä–µ—á–∞—é—â–∏–µ—Å—è –º–µ–Ω—å—à–µ —Ä–∞–∑\n",
    "    ngram_range=(1,1)  # 1-–≥—Ä–∞–º–º—ã (–æ—Ç–¥–µ–ª—å–Ω—ã–µ —Å–ª–æ–≤–∞)\n",
    ")\n",
    "\n",
    "# –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Ç–µ–∫—Å—Ç—ã –≤ –º–∞—Ç—Ä–∏—Ü—É —á–∞—Å—Ç–æ—Ç\n",
    "X = vectorizer.fit_transform(texts)\n",
    "\n",
    "# –°–ª–æ–≤–∞—Ä—å —Å–ª–æ–≤ (—Ñ–∏—á–∏)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "print(\"–°–ª–æ–≤–∞—Ä—å —Å–ª–æ–≤:\")\n",
    "print(feature_names)\n",
    "\n",
    "# –í–µ–∫—Ç–æ—Ä –¥–æ–∫—É–º–µ–Ω—Ç–∞\n",
    "print(\"\\n–ú–∞—Ç—Ä–∏—Ü–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (CountVectorizer):\")\n",
    "print(X.toarray())\n",
    "\n",
    "# –ö–∞–∂–¥–∞—è —Å—Ç—Ä–æ–∫–∞ = –¥–æ–∫—É–º–µ–Ω—Ç, –∫–∞–∂–¥–∞—è –∫–æ–ª–æ–Ω–∫–∞ = —Å–ª–æ–≤–æ, —á–∏—Å–ª–æ = –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–∞–∑, –∫–æ—Ç–æ—Ä–æ–µ —Å–ª–æ–≤–æ –≤—Å—Ç—Ä–µ—Ç–∏–ª–æ—Å—å\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
